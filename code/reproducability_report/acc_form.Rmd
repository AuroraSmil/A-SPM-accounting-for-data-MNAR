<!--HOW TO COMPLETE THIS FORM:-->

<!--
1. Checkboxes in this document appear as follows: 

- [ ] This is a checkbox 

To check a checkbox, replace [ ] by [x], as follows: 

- [x] This is a checked checkbox 

Note that older versions of RStudio (versions lower than 1.3) may not create a formatted checkbox but will leave the original characters, i.e., literally "[ ]" or "[x]". It's fine to submit a PDF in this form.
 
2. For text answers, simply type the relevant text in the areas indicated. A blank line starts a new paragraph. 
 
3. Comments (like these instructions) provide additional instructions throughout the form. There is no need to remove them; they will not appear in the compiled document. 

4. If you are comfortable with Markdown syntax, you may choose to include any Markdown-compliant formatting in the form. For example, you may wish to include R code chunks and compile this document in R Markdown.
-->

This form documents the artifacts associated with the article (i.e., the data and code supporting the computational findings) and describes how to reproduce the findings.


# Part 1: Data

- [ ] This paper does not involve analysis of external data (i.e., no data are used or the only data are generated by the authors via simulation in their code).

<!--
If box above is checked and if no simulated/synthetic data files are provided by the authors, please skip directly to the Code section. Otherwise, continue.
-->

- [x] I certify that the author(s) of the manuscript have legitimate access to and permission to use the data used in this manuscript.

<!-- If data are simulated using random number generation, please be sure to set the random number seed in the code you provide -->

## Abstract

<!--
Provide a short (< 100 words), high-level description of the data
-->

The HUNT Study is a longitudinal population-based health survey in central Norway \citep{aasvold2021cohort}. Every adult citizen in the now former county of Nord-Trøndelag was invited to participate in the first survey in 1984-86 (HUNT1) and all adult residents in the screening area have since then been invited to clinical examinations and questionnaires in 1995-97 (HUNT2), 2006-08 (HUNT3), and 2017-19 (HUNT4).
This work uses HUNT data in a predictive model of BP eleven years ahead ($BP_f$) based on current BP ($BP_c$), $age$, $sex$, and $BMI$. 
As large proportions ($33\% -43\%$) of participants are lost to follow-up in between consecutive survayse we want to account for data being missing not at random (MNAR). Hence we construct the variable $m$ indicating if a participant is lost to follow-up ($m=1$) or not ($m= 0$). 
Hence the dataset contains the following variables: $BP_f$, $m$, $sex$, $age$, $BMI$ and $BP_c$.

## Availability


- [ ] Data **are** publicly available.
- [x] Data **cannot be made** publicly available.

If the data are publicly available, see the *Publicly available data* section. Otherwise, see the *Non-publicly available data* section, below.

### Publicly available data

- [ ] Data are available online at:

- [ ] Data are available as part of the paper’s supplementary material.

- [ ] Data are publicly available by request, following the process described here:

- [ ] Data are or will be made available through some other mechanism, described here:


<!-- If data are available by request to the authors or some other data owner, please make sure to explain the process of requesting access to the data. -->

### Non-publicly available data

<!--
The Journal of the American Statistical Association requires authors to make data accompanying their papers available to the scientific community except in cases where: 1) public sharing of data would be impossible, 2) suitable synthetic data are provided which allow the main analyses to be replicated (recognizing that results may differ from the "real" data analyses), and 3) the scientific value of the results and methods outweigh the lack of reproducibility.

Please discuss the lack of publicly available data. For example:
-	why data sharing is not possible,
-	what synthetic data are provided, and 
-	why the value of the paper's scientific contribution outweighs the lack of reproducibility.
-->

The HUNT data containes sensitive information and is protected by law. The following paragraph is from HUNTs guidelines for publication \url{https://www.ntnu.edu/documents/140075/1295406997/Guidelines+for+publication+of+research+results+using+HUNT-data.pdf/007a5d98-4369-94a6-06b7-6264367b3faa?t=1600862719853}:

"The Trøndelag Health Study (HUNT) has invited persons aged 13 - 100 years to four surveys between 1984
and 2019. Comprehensive data from more than 140,000 persons having participated at least once and
biological material from 78,000 persons are collected. The data are stored in HUNT databank and biological
material in HUNT biobank. HUNT Research Centre has permission from the Norwegian Data Inspectorate
to store and handle these data. The key identification in the data base is the personal identification number
given to all Norwegians at birth or immigration, whilst de-identified data are sent to researchers upon
approval of a research protocol by the Regional Ethical Committee and HUNT Research Centre. To protect
participants’ privacy, HUNT Research Centre aims to limit storage of data outside HUNT databank, and
cannot deposit data in open repositories. HUNT databank has precise information on all data exported to
different projects and are able to reproduce these on request. There are no restrictions regarding data export
given approval of applications to HUNT Research Centre. For more information see:
http://www.ntnu.edu/hunt/data"

Therefor we provide syntetic data with similar properties containing the same variables. This ensures all code can be run and inspected. 
However the results will not be identical to those producen on the real data. 

Still we find if of scientific interest to explore how the models used in this work on real data and believe this can improve our understanding of blood pressure and how to make good predictive models for this ocndition. 


## Description

### File format(s)

<!--
Check all that apply
-->
- [x] CSV or other plain text.
- [ ] Software-specific binary format (.Rda, Python pickle, etc.): pkcle
- [ ] Standardized binary format (e.g., netCDF, HDF5, etc.): 
- [ ] Other (please specify):

### Data dictionary

<!--
A data dictionary provides information that allows users to understand the meaning, format, and use of the data.
-->

- [ ] Provided by authors in the following file(s):
- [x] Data file(s) is(are) self-describing (e.g., netCDF files)
- [ ] Available at the following URL: 

### Additional Information (optional)
<!-- 
OPTIONAL: Provide any additional details that would be helpful in understanding the data. If relevant, please provide unique identifier/DOI/version information and/or license/terms of use.
-->
We have created to simulated datasets. One of the same size as the original data and one very small dataset. To run the code on a dataset of original size one needs a lot of computational power. We had 32 CPUs and 32 G RAM available. The small dataset can however be run on a desctop laptop to inspect the code. We do note that the small dataset will not provide even remotly similar results as the true data because the models used need datasets of certain sizes to establish the proper connections. 


# Part 2: Code

## Abstract

<!--
Provide a short (< 100 words), high-level description of the code. If necessary, more details can be provided in files that accompany the code. If no code is provided, please state this and say why (e.g., if the paper contains no computational work).
-->

## Description

### Code format(s)

<!--
Check all that apply
-->
- [ ] Script files
    - [x] R
    - [ ] Python
    - [ ] Matlab
    - [ ] Other: 
- [ ] Package
    - [x] R
    - [ ] Python
    - [ ] MATLAB toolbox
    - [ ] Other: 
- [ ] Reproducible report 
    - [ ] R Markdown
    - [ ] Jupyter notebook
    - [ ] Other:
- [ ] Shell script
- [ ] Other (please specify): 

### Supporting software requirements

#### Version of primary software used

R version 4.0.4 (2021-02-15)

#### Libraries and dependencies used by the code


<!--
Include version numbers (e.g., version numbers for any R or Python packages used)
-->
 [1] glue_1.4.2          lubridate_1.7.9.2   tikzDevice_0.12.3.1 latex2exp_0.4.0    
 [5] data.table_1.13.6   forcats_0.5.1       stringr_1.4.0       dplyr_1.0.4        
 [9] purrr_0.3.4         readr_1.4.0         tidyr_1.1.2         tibble_3.0.6       
[13] ggplot2_3.3.3       tidyverse_1.3.0     brinla_0.1.0        INLA_21.02.23      
[17] sp_1.4-5            foreach_1.5.1       Matrix_1.3-2      
### Supporting system/hardware requirements (optional)

<!--
OPTIONAL: System/hardware requirements including operating system with version number, access to cluster, GPUs, etc.
-->

Ubuntu 18.04.6 LTS

### Parallelization used

- [ ] No parallel code used
- [ x] Multi-core parallelization on a single machine/node
    - Number of cores used: 32 vCPU
- [ ] Multi-machine/multi-node parallelization 
    - Number of nodes and cores used: 

### License

- [x] MIT License (default)
- [ ] BSD 
- [ ] GPL v3.0
- [ ] Creative Commons
- [ ] Other: (please specify)

### Additional information (optional)

<!--
OPTIONAL: By default, submitted code will be published on the JASA GitHub repository (http://github.com/JASA-ACS) as well as in the supplementary material. Authors are encouraged to also make their code available in a public code repository, such as on GitHub, GitLab, or BitBucket. If relevant, please provide unique identifier/DOI/version information (e.g., a Git commit ID, branch, release, or tag). If the code and workflow are provided together, this section may be omitted, with information provided in the "Location" section below.
-->

# Part 3: Reproducibility workflow

<!--
The materials provided should provide a straightforward way for reviewers and readers to reproduce analyses with as few steps as possible. 
-->

## Scope

The provided workflow reproduces:

- [ ] Any numbers provided in text in the paper
- [x] The computational method(s) presented in the paper (i.e., code is provided that implements the method(s))
- [x] All tables and figures in the paper
- [ ] Selected tables and figures in the paper, as explained and justified below:

We note that the report will not provide identical results due to the use of simulated data. 

## Workflow

### Location

The workflow is available:

<!--
Check all that apply, and in the case of a Git repository include unique identifier, such as specific commit ID, branch, release, or tag.
-->
- [ ] As part of the paper’s supplementary material.
- [x] In this Git repository:
- [ ] Other (please specify):

<!--
Indicate where the materials (generally including the code, unless in a separate location and indicated in the previous section) are available. We strongly encourage authors to place their materials (but not large datasets) in a Git repository hosted on a site such as GitHub, GitLab, or BitBucket. If the repository is private during the review process, please indicate the location where it will be available publicly upon publication, and also include the materials as a zip file (e.g., obtained directly from the Git hosting site) as supplementary materials.
-->


### Format(s)

<!--
Check all that apply
-->
- [ ] Single master code file 
- [ ] Wrapper (shell) script(s)
- [ ] Self-contained R Markdown file, Jupyter notebook, or other literate programming approach
- [ ] Text file (e.g., a readme-style file) that documents workflow
- [ ] Makefile
- [ ] Other (more detail in *Instructions* below)







### Instructions

<!--
Describe how to use the materials provided to reproduce analyses in the manuscript. Additional details can be provided in file(s) accompanying the reproducibility materials. If no workflow is provided, please state this and say why (e.g., if the paper contains no computational work).
-->

### Expected run-time

Approximate time needed to reproduce the analyses on a standard desktop machine:

- [ ] < 1 minute
- [ ] 1-10 minutes
- [ ] 10-60 minutes
- [ ] 1-8 hours
- [ ] > 8 hours
- [x] Not feasible to run on a desktop machine, as described here:

To use the dataset of full size one needs at least 32 CPU and 32 G Ram. 
To run the small dataset a it is possible to run most of the code ona normal desctop computer.

### Additional information (optional)

<!--
OPTIONAL: Additional documentation provided (e.g., R package vignettes, demos or other examples) that show how to use the provided code/software in other settings.
-->

# Notes (optional)

<!--
OPTIONAL: Any other relevant information not covered on this form. If reproducibility materials are not publicly available at the time of submission, please provide information here on how the reviewers can view the materials.
-->
